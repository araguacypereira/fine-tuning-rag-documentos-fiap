{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVEwjMHq8bli",
        "outputId": "5c6700df-f866-4380-d612-f2f10fdcdc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Conexão com o Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W5XK4Eg88Nnr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def get_news_content(links_file):\n",
        "    # Carrega o arquivo com os links e retorna uma lista\n",
        "    with open(links_file, 'r') as file:\n",
        "        links = set(file.readlines())\n",
        "    news_contents = []\n",
        "\n",
        "    for link in links:\n",
        "        link = link.strip()  # Limpa a URL, se necessário\n",
        "        print(link)\n",
        "\n",
        "        response = requests.get(link)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Localiza o elemento que contém o conteúdo do artigo\n",
        "            article_content = soup.find('div', class_='article__content')\n",
        "            if article_content:\n",
        "                #paragraphs = article_content.find_all('p', class_='paragraph inline-placeholder')\n",
        "                paragraphs = article_content.find_all('p', class_='paragraph-elevate inline-placeholder vossi-paragraph')\n",
        "                # Concatena o texto de todos os parágrafos encontrados\n",
        "                content = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
        "                news_contents.append(content)\n",
        "                #print(content)\n",
        "            else:\n",
        "                news_contents.append(\"Conteúdo não encontrado.\")\n",
        "        else:\n",
        "            news_contents.append(f\"Falha ao extrair o conteúdo da notícia: {response.status_code}\")\n",
        "\n",
        "    # Salva o conteúdo em um arquivo JSON\n",
        "    path_json = '/content/drive/MyDrive/Fiap/news_contents.json' # Substitua se quiser salvar em uma pasta específica\n",
        "    with open(path_json, 'w') as json_file:\n",
        "        json.dump({\"news_content\": news_contents}, json_file)\n",
        "\n",
        "# Chamada da função para extrair os conteúdos\n",
        "drive_path = '/content/drive/MyDrive/Fiap/CNN_Links.txt' # Substitua se quiser salvar em uma pasta específica\n",
        "get_news_content(drive_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qlY_dM6LMDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}